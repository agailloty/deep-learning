{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXAUKUwxQx5HGo8MwFFSSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/agailloty/deep-learning/blob/main/pytorch/pytorch_to_onnx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FcDze8T3-C2a",
        "outputId": "caa5d546-f58f-4e88-aac8-c9ded10f6531"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,    10] loss: 0.738\n",
            "[2,    10] loss: 0.707\n",
            "[3,    10] loss: 0.689\n",
            "[4,    10] loss: 0.674\n",
            "[5,    10] loss: 0.669\n",
            "[6,    10] loss: 0.665\n",
            "[7,    10] loss: 0.662\n",
            "[8,    10] loss: 0.661\n",
            "[9,    10] loss: 0.659\n",
            "[10,    10] loss: 0.656\n",
            "[11,    10] loss: 0.659\n",
            "[12,    10] loss: 0.658\n",
            "[13,    10] loss: 0.661\n",
            "[14,    10] loss: 0.658\n",
            "[15,    10] loss: 0.658\n",
            "[16,    10] loss: 0.657\n",
            "[17,    10] loss: 0.656\n",
            "[18,    10] loss: 0.656\n",
            "[19,    10] loss: 0.655\n",
            "[20,    10] loss: 0.657\n",
            "[21,    10] loss: 0.657\n",
            "[22,    10] loss: 0.656\n",
            "[23,    10] loss: 0.659\n",
            "[24,    10] loss: 0.656\n",
            "[25,    10] loss: 0.658\n",
            "[26,    10] loss: 0.660\n",
            "[27,    10] loss: 0.656\n",
            "[28,    10] loss: 0.655\n",
            "[29,    10] loss: 0.655\n",
            "[30,    10] loss: 0.657\n",
            "[31,    10] loss: 0.661\n",
            "[32,    10] loss: 0.657\n",
            "[33,    10] loss: 0.657\n",
            "[34,    10] loss: 0.656\n",
            "[35,    10] loss: 0.657\n",
            "[36,    10] loss: 0.658\n",
            "[37,    10] loss: 0.655\n",
            "[38,    10] loss: 0.656\n",
            "[39,    10] loss: 0.657\n",
            "[40,    10] loss: 0.656\n",
            "[41,    10] loss: 0.656\n",
            "[42,    10] loss: 0.659\n",
            "[43,    10] loss: 0.654\n",
            "[44,    10] loss: 0.657\n",
            "[45,    10] loss: 0.656\n",
            "[46,    10] loss: 0.655\n",
            "[47,    10] loss: 0.654\n",
            "[48,    10] loss: 0.655\n",
            "[49,    10] loss: 0.654\n",
            "[50,    10] loss: 0.659\n",
            "[51,    10] loss: 0.657\n",
            "[52,    10] loss: 0.654\n",
            "[53,    10] loss: 0.654\n",
            "[54,    10] loss: 0.658\n",
            "[55,    10] loss: 0.656\n",
            "[56,    10] loss: 0.656\n",
            "[57,    10] loss: 0.656\n",
            "[58,    10] loss: 0.657\n",
            "[59,    10] loss: 0.653\n",
            "[60,    10] loss: 0.660\n",
            "[61,    10] loss: 0.655\n",
            "[62,    10] loss: 0.656\n",
            "[63,    10] loss: 0.658\n",
            "[64,    10] loss: 0.656\n",
            "[65,    10] loss: 0.659\n",
            "[66,    10] loss: 0.656\n",
            "[67,    10] loss: 0.657\n",
            "[68,    10] loss: 0.656\n",
            "[69,    10] loss: 0.657\n",
            "[70,    10] loss: 0.656\n",
            "[71,    10] loss: 0.657\n",
            "[72,    10] loss: 0.656\n",
            "[73,    10] loss: 0.658\n",
            "[74,    10] loss: 0.655\n",
            "[75,    10] loss: 0.656\n",
            "[76,    10] loss: 0.655\n",
            "[77,    10] loss: 0.653\n",
            "[78,    10] loss: 0.655\n",
            "[79,    10] loss: 0.657\n",
            "[80,    10] loss: 0.658\n",
            "[81,    10] loss: 0.657\n",
            "[82,    10] loss: 0.658\n",
            "[83,    10] loss: 0.656\n",
            "[84,    10] loss: 0.654\n",
            "[85,    10] loss: 0.657\n",
            "[86,    10] loss: 0.656\n",
            "[87,    10] loss: 0.659\n",
            "[88,    10] loss: 0.653\n",
            "[89,    10] loss: 0.654\n",
            "[90,    10] loss: 0.654\n",
            "[91,    10] loss: 0.656\n",
            "[92,    10] loss: 0.654\n",
            "[93,    10] loss: 0.657\n",
            "[94,    10] loss: 0.658\n",
            "[95,    10] loss: 0.656\n",
            "[96,    10] loss: 0.657\n",
            "[97,    10] loss: 0.654\n",
            "[98,    10] loss: 0.656\n",
            "[99,    10] loss: 0.656\n",
            "[100,    10] loss: 0.655\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Définir les données d'entraînement\n",
        "X_train = torch.randn(100, 10)  # exemple de données d'entrée\n",
        "y_train = torch.randint(0, 2, (100,))  # exemple de données de sortie (0 ou 1)\n",
        "\n",
        "# Créer un DataLoader à partir des données d'entraînement\n",
        "train_data = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_data, batch_size=10, shuffle=True)\n",
        "\n",
        "# Définir le modèle de régression logistique\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = torch.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "model = LogisticRegression(input_size=10, output_size=1)\n",
        "\n",
        "# Définir la fonction de perte et l'optimiseur\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "\n",
        "# Entraîner le modèle\n",
        "for epoch in range(100):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels.float().view(-1, 1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 10 == 9:\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 10))\n",
        "            running_loss = 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hDhMaPPrBiDY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}